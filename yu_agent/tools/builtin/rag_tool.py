"""RAGå·¥å…· - æ£€ç´¢å¢å¼ºç”Ÿæˆ

ä¸ºHelloAgentsæ¡†æ¶æä¾›ç®€æ´æ˜“ç”¨çš„RAGèƒ½åŠ›ï¼š
- ğŸ”„ æ•°æ®æµç¨‹ï¼šç”¨æˆ·æ•°æ® â†’ æ–‡æ¡£è§£æ â†’ å‘é‡åŒ–å­˜å‚¨ â†’ æ™ºèƒ½æ£€ç´¢ â†’ LLMå¢å¼ºé—®ç­”
- ğŸ“š å¤šæ ¼å¼æ”¯æŒï¼šPDFã€Wordã€Excelã€PPTã€å›¾ç‰‡ã€éŸ³é¢‘ã€ç½‘é¡µç­‰
- ğŸ§  æ™ºèƒ½é—®ç­”ï¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³å†…å®¹ï¼Œæ³¨å…¥æç¤ºè¯ï¼Œç”Ÿæˆå‡†ç¡®ç­”æ¡ˆ
- ğŸ·ï¸ å‘½åç©ºé—´ï¼šæ”¯æŒå¤šé¡¹ç›®éš”ç¦»ï¼Œä¾¿äºç®¡ç†ä¸åŒçŸ¥è¯†åº“

ä½¿ç”¨ç¤ºä¾‹ï¼š
```python
# 1. åˆå§‹åŒ–RAGå·¥å…·
rag = RAGTool()

# 2. æ·»åŠ æ–‡æ¡£
rag.run({"action": "add_document", "file_path": "document.pdf"})

# 3. æ™ºèƒ½é—®ç­”
answer = rag.run({"action": "ask", "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"})
```
"""

from typing import Dict, Any, List, Optional
import os
import time

from ..base import Tool, ToolParameter
from ...memory.rag.pipeline import create_rag_pipeline
from ...core.llm import AgentsLLM

class RAGTool(Tool):
    """RAGå·¥å…·
    
    æä¾›å®Œæ•´çš„ RAG èƒ½åŠ›ï¼š
    - æ·»åŠ å¤šæ ¼å¼æ–‡æ¡£ï¼ˆPDFã€Officeã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰ï¼‰
    - æ™ºèƒ½æ£€ç´¢ä¸å¬å›
    - LLM å¢å¼ºé—®ç­”
    - çŸ¥è¯†åº“ç®¡ç†
    """
    
    def __init__(
        self,
        knowledge_base_path: str = "./knowledge_base",
        qdrant_url: str = None,
        qdrant_api_key: str = None,
        collection_name: str = "rag_knowledge_base",
        rag_namespace: str = "default"
    ):
        super().__init__(
            name="rag",
            description="RAGå·¥å…· - æ”¯æŒå¤šæ ¼å¼æ–‡æ¡£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæä¾›æ™ºèƒ½é—®ç­”èƒ½åŠ›"
        )
        
        self.knowledge_base_path = knowledge_base_path
        self.qdrant_url = qdrant_url or os.getenv("QDRANT_URL")
        self.qdrant_api_key = qdrant_api_key or os.getenv("QDRANT_API_KEY")
        self.collection_name = collection_name
        self.rag_namespace = rag_namespace
        self._pipelines: Dict[str, Dict[str, Any]] = {}
        
        # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
        os.makedirs(knowledge_base_path, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–RAGç»„ä»¶"""
        try:
            # åˆå§‹åŒ–é»˜è®¤å‘½åç©ºé—´çš„ RAG ç®¡é“
            default_pipeline = create_rag_pipeline(
                qdrant_url=self.qdrant_url,
                qdrant_api_key=self.qdrant_api_key,
                collection_name=self.collection_name,
                rag_namespace=self.rag_namespace
            )
            self._pipelines[self.rag_namespace] = default_pipeline

            # åˆå§‹åŒ– LLM ç”¨äºå›ç­”ç”Ÿæˆ
            self.llm = AgentsLLM()

            self.initialized = True
            print(f"âœ… RAGå·¥å…·åˆå§‹åŒ–æˆåŠŸ: namespace={self.rag_namespace}, collection={self.collection_name}")
            
        except Exception as e:
            self.initialized = False
            self.init_error = str(e)
            print(f"âŒ RAGå·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")

    def _get_pipeline(self, namespace: Optional[str] = None) -> Dict[str, Any]:
        """è·å–æŒ‡å®šå‘½åç©ºé—´çš„ RAG ç®¡é“ï¼Œè‹¥ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º"""
        target_ns = namespace or self.rag_namespace
        if target_ns in self._pipelines:
            return self._pipelines[target_ns]

        pipeline = create_rag_pipeline(
            qdrant_url=self.qdrant_url,
            qdrant_api_key=self.qdrant_api_key,
            collection_name=self.collection_name,
            rag_namespace=target_ns
        )
        self._pipelines[target_ns] = pipeline
        return pipeline

    def run(self, parameters: Dict[str, Any]) -> str:
        """æ‰§è¡Œå·¥å…· - ToolåŸºç±»è¦æ±‚çš„æ¥å£

        Args:
            parameters: å·¥å…·å‚æ•°å­—å…¸ï¼Œå¿…é¡»åŒ…å«actionå‚æ•°

        Returns:
            æ‰§è¡Œç»“æœå­—ç¬¦ä¸²
        """
        if not self.validate_parameters(parameters):
            return "âŒ å‚æ•°éªŒè¯å¤±è´¥ï¼šç¼ºå°‘å¿…éœ€çš„å‚æ•°"

        action = parameters.get("action")
        # ç§»é™¤actionå‚æ•°ï¼Œä¼ é€’å…¶ä½™å‚æ•°ç»™executeæ–¹æ³•
        kwargs = {k: v for k, v in parameters.items() if k != "action"}

        return self.execute(action, **kwargs)

    def get_parameters(self) -> List[ToolParameter]:
        """è·å–å·¥å…·å‚æ•°å®šä¹‰ - ToolåŸºç±»è¦æ±‚çš„æ¥å£"""
        return [
            # æ ¸å¿ƒæ“ä½œå‚æ•°
            ToolParameter(
                name="action",
                type="string",
                description="æ“ä½œç±»å‹ï¼šadd_document(æ·»åŠ æ–‡æ¡£), add_text(æ·»åŠ æ–‡æœ¬), ask(æ™ºèƒ½é—®ç­”), search(æœç´¢), stats(ç»Ÿè®¡), clear(æ¸…ç©º)",
                required=True
            ),
            
            # å†…å®¹å‚æ•°
            ToolParameter(
                name="file_path",
                type="string",
                description="æ–‡æ¡£æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒPDFã€Wordã€Excelã€PPTã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰å¤šç§æ ¼å¼ï¼‰",
                required=False
            ),
            ToolParameter(
                name="text",
                type="string",
                description="è¦æ·»åŠ çš„æ–‡æœ¬å†…å®¹",
                required=False
            ),
            ToolParameter(
                name="question",
                type="string", 
                description="ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äºæ™ºèƒ½é—®ç­”ï¼‰",
                required=False
            ),
            ToolParameter(
                name="query",
                type="string",
                description="æœç´¢æŸ¥è¯¢è¯ï¼ˆç”¨äºåŸºç¡€æœç´¢ï¼‰",
                required=False
            ),
            
            # å¯é€‰é…ç½®å‚æ•°
            ToolParameter(
                name="namespace",
                type="string",
                description="çŸ¥è¯†åº“å‘½åç©ºé—´ï¼ˆç”¨äºéš”ç¦»ä¸åŒé¡¹ç›®ï¼Œé»˜è®¤ï¼šdefaultï¼‰",
                required=False,
                default="default"
            ),
            ToolParameter(
                name="limit",
                type="integer",
                description="è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ï¼š5ï¼‰",
                required=False,
                default=5
            ),
            ToolParameter(
                name="include_citations",
                type="boolean",
                description="æ˜¯å¦åŒ…å«å¼•ç”¨æ¥æºï¼ˆé»˜è®¤ï¼štrueï¼‰",
                required=False,
                default=True
            )
        ]
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒRAGæ“ä½œ
        
        ä¸»è¦æ“ä½œæµç¨‹ï¼š
        1. add_document/add_text: æ•°æ® â†’ è§£æ â†’ åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨
        2. ask: é—®é¢˜ â†’ æ£€ç´¢ â†’ ä¸Šä¸‹æ–‡æ³¨å…¥ â†’ LLMç”Ÿæˆç­”æ¡ˆ
        3. search: æŸ¥è¯¢ â†’ å‘é‡æ£€ç´¢ â†’ è¿”å›ç›¸å…³ç‰‡æ®µ
        """
        
        if not self.initialized:
            return f"âŒ RAGå·¥å…·æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®: {getattr(self, 'init_error', 'æœªçŸ¥é”™è¯¯')}"
        
        # å‚æ•°é¢„å¤„ç†
        kwargs = self._preprocess_parameters(action, **kwargs)
        
        try:
            if action == "add_document":
                return self._add_document(**kwargs)
            elif action == "add_text":
                return self._add_text(**kwargs)
            elif action == "ask":
                return self._ask(**kwargs)
            elif action == "search":
                return self._search(**kwargs)
            elif action == "stats":
                return self._get_stats(namespace=kwargs.get("namespace"))
            elif action == "clear":
                return self._clear_knowledge_base(**kwargs)
            else:
                available_actions = ["add_document", "add_text", "ask", "search", "stats", "clear"]
                return f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: {action}\nâœ… å¯ç”¨æ“ä½œ: {', '.join(available_actions)}"
                
        except Exception as e:
            return f"âŒ æ‰§è¡Œæ“ä½œ '{action}' æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def _preprocess_parameters(self, action: str, **kwargs) -> Dict[str, Any]:
        """é¢„å¤„ç†å‚æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼å’ŒéªŒè¯"""
        # è®¾ç½®é»˜è®¤å€¼
        defaults = {
            "namespace": "default",
            "limit": 5,
            "include_citations": True,
            "enable_advanced_search": True,
            "max_chars": 1200,
            "min_score": 0.1,
            "chunk_size": 800,
            "chunk_overlap": 100
        }
        
        for key, value in defaults.items():
            if key not in kwargs or kwargs[key] is None:
                kwargs[key] = value
        
        # å‚æ•°éªŒè¯
        if action in ["add_document"] and not kwargs.get("file_path"):
            raise ValueError("add_document æ“ä½œéœ€è¦æä¾› file_path å‚æ•°")
        elif action in ["add_text"] and not kwargs.get("text"):
            raise ValueError("add_text æ“ä½œéœ€è¦æä¾› text å‚æ•°")
        elif action in ["ask"] and not (kwargs.get("question") or kwargs.get("query")):
            raise ValueError("ask æ“ä½œéœ€è¦æä¾› question æˆ– query å‚æ•°")
        elif action in ["search"] and not (kwargs.get("query") or kwargs.get("question")):
            raise ValueError("search æ“ä½œéœ€è¦æä¾› query æˆ– question å‚æ•°")
            
        return kwargs

    def _add_document(self, file_path: str, document_id: str = None, namespace: Optional[str] = None, chunk_size: int = 800, chunk_overlap: int = 100, **kwargs) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆæ”¯æŒå¤šæ ¼å¼ï¼‰"""
        try:
            if not file_path or not os.path.exists(file_path):
                return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            
            pipeline = self._get_pipeline(namespace)
            t0 = time.time()

            chunks_added = pipeline["add_documents"](
                file_paths=[file_path],
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            t1 = time.time()
            process_ms = int((t1 - t0) * 1000)
            
            if chunks_added == 0:
                return f"âš ï¸ æœªèƒ½ä»æ–‡ä»¶è§£æå†…å®¹: {os.path.basename(file_path)}"
            
            return (
                f"âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°çŸ¥è¯†åº“: {os.path.basename(file_path)}\n"
                f"ğŸ“Š åˆ†å—æ•°é‡: {chunks_added}\n"
                f"â±ï¸ å¤„ç†æ—¶é—´: {process_ms}ms\n"
                f"ğŸ“ å‘½åç©ºé—´: {pipeline.get('namespace', self.rag_namespace)}"
            )
            
        except Exception as e:
            return f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}"
    
    def _add_text(self, text: str, document_id: str = None, metadata: Optional[Dict[str, Any]] = None, namespace: Optional[str] = None, chunk_size: int = 800, chunk_overlap: int = 100, **kwargs) -> str:
        """æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“"""
        try:
            if not text or not text.strip():
                return "âŒ æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º"
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            document_id = document_id or f"text_{abs(hash(text)) % 100000}"
            tmp_path = os.path.join(self.knowledge_base_path, f"{document_id}.md")
            
            try:
                with open(tmp_path, 'w', encoding='utf-8') as f:
                    f.write(text)
                
                pipeline = self._get_pipeline(namespace)
                t0 = time.time()

                chunks_added = pipeline["add_documents"](
                    file_paths=[tmp_path],
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                
                t1 = time.time()
                process_ms = int((t1 - t0) * 1000)
                
                if chunks_added == 0:
                    return f"âš ï¸ æœªèƒ½ä»æ–‡æœ¬ç”Ÿæˆæœ‰æ•ˆåˆ†å—"
                
                return (
                    f"âœ… æ–‡æœ¬å·²æ·»åŠ åˆ°çŸ¥è¯†åº“: {document_id}\n"
                    f"ğŸ“Š åˆ†å—æ•°é‡: {chunks_added}\n"
                    f"â±ï¸ å¤„ç†æ—¶é—´: {process_ms}ms\n"
                    f"ğŸ“ å‘½åç©ºé—´: {pipeline.get('namespace', self.rag_namespace)}"
                )
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if os.path.exists(tmp_path):
                        os.remove(tmp_path)
                except Exception:
                    pass
            
        except Exception as e:
            return f"âŒ æ·»åŠ æ–‡æœ¬å¤±è´¥: {str(e)}"
    
    def _search(self, query: str, limit: int = 5, min_score: float = 0.1, enable_advanced_search: bool = True, max_chars: int = 1200, include_citations: bool = True, namespace: Optional[str] = None, **kwargs) -> str:
        """æœç´¢çŸ¥è¯†åº“"""
        try:
            if not query or not query.strip():
                return "âŒ æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"
            
            # ä½¿ç”¨ç»Ÿä¸€ RAG ç®¡é“æœç´¢
            pipeline = self._get_pipeline(namespace)

            if enable_advanced_search:
                results = pipeline["search_advanced"](
                    query=query,
                    top_k=limit,
                    enable_mqe=True,
                    enable_hyde=True,
                    score_threshold=min_score if min_score > 0 else None
                )
            else:
                results = pipeline["search"](
                    query=query,
                    top_k=limit,
                    score_threshold=min_score if min_score > 0 else None
                )
            
            if not results:
                return f"ğŸ” æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„å†…å®¹"
            
            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            search_result = ["æœç´¢ç»“æœï¼š"]
            for i, result in enumerate(results, 1):
                meta = result.get("metadata", {})
                score = result.get("score", 0.0)
                content = meta.get("content", "")[:200] + "..."
                source = meta.get("source_path", "unknown")
                
                # å®‰å…¨å¤„ç†Unicode
                def clean_text(text):
                    try:
                        return str(text).encode('utf-8', errors='ignore').decode('utf-8')
                    except Exception:
                        return str(text)
                
                clean_content = clean_text(content)
                clean_source = clean_text(source)
                
                search_result.append(f"\n{i}. æ–‡æ¡£: **{clean_source}** (ç›¸ä¼¼åº¦: {score:.3f})")
                search_result.append(f"   {clean_content}")
                
                if include_citations and meta.get("heading_path"):
                    clean_heading = clean_text(str(meta['heading_path']))
                    search_result.append(f"   ç« èŠ‚: {clean_heading}")
            
            return "\n".join(search_result)
            
        except Exception as e:
            return f"âŒ æœç´¢å¤±è´¥: {str(e)}"
    
    def _ask(self, question: Optional[str] = None, query: Optional[str] = None, limit: int = 5, enable_advanced_search: bool = True, include_citations: bool = True, max_chars: int = 1200, namespace: Optional[str] = None, **kwargs) -> str:
        """æ™ºèƒ½é—®ç­”ï¼šæ£€ç´¢ â†’ ä¸Šä¸‹æ–‡æ³¨å…¥ â†’ LLMç”Ÿæˆç­”æ¡ˆ
        
        æ ¸å¿ƒæµç¨‹ï¼š
        1. è§£æç”¨æˆ·é—®é¢˜
        2. æ™ºèƒ½æ£€ç´¢ç›¸å…³å†…å®¹
        3. æ„å»ºä¸Šä¸‹æ–‡å’Œæç¤ºè¯
        4. LLMç”Ÿæˆå‡†ç¡®ç­”æ¡ˆ
        5. æ·»åŠ å¼•ç”¨æ¥æº
        """
        try:
            # è·å–ç”¨æˆ·é—®é¢˜ï¼ˆquestion ä¼˜å…ˆçº§é«˜äº queryï¼‰
            user_question = question or query
            if not user_question or not user_question.strip():
                return "âŒ è¯·æä¾›è¦è¯¢é—®çš„é—®é¢˜"
            
            user_question = user_question.strip()
            print(f"ğŸ” æ™ºèƒ½é—®ç­”: {user_question}")
            
            # 1. æ£€ç´¢ç›¸å…³å†…å®¹
            pipeline = self._get_pipeline(namespace)
            search_start = time.time()
            
            if enable_advanced_search:
                results = pipeline["search_advanced"](
                    query=user_question,
                    top_k=limit,
                    enable_mqe=True,
                    enable_hyde=True
                )
            else:
                results = pipeline["search"](
                    query=user_question,
                    top_k=limit
                )
            
            search_time = int((time.time() - search_start) * 1000)
            
            if not results:
                return (
                    f"ğŸ¤” æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{user_question}ã€ç›¸å…³çš„ä¿¡æ¯ã€‚\n\n"
                    f"ğŸ’¡ å»ºè®®ï¼š\n"
                    f"â€¢ å°è¯•ä½¿ç”¨æ›´ç®€æ´çš„å…³é”®è¯\n"
                    f"â€¢ æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ ç›¸å…³æ–‡æ¡£\n"
                    f"â€¢ ä½¿ç”¨ stats æ“ä½œæŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€"
                )
            
            # 2. æ™ºèƒ½æ•´ç†ä¸Šä¸‹æ–‡
            context_parts = []
            citations = []
            total_score = 0
            
            for i, result in enumerate(results):
                meta = result.get("metadata", {})
                content = meta.get("content", "").strip()
                source = meta.get("source_path", "unknown")
                score = result.get("score", 0.0)
                total_score += score
                
                if content:
                    # æ¸…ç†å†…å®¹æ ¼å¼
                    cleaned_content = self._clean_content_for_context(content)
                    context_parts.append(f"ç‰‡æ®µ {i+1}ï¼š{cleaned_content}")
                    
                    if include_citations:
                        citations.append({
                            "index": i+1,
                            "source": os.path.basename(source),
                            "score": score
                        })
            
            # 3. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ™ºèƒ½æˆªæ–­ï¼‰
            context = "\n\n".join(context_parts)
            if len(context) > max_chars:
                # æ™ºèƒ½æˆªæ–­ï¼Œä¿æŒå®Œæ•´æ€§
                context = self._smart_truncate_context(context, max_chars)
            
            # 4. æ„å»ºå¢å¼ºæç¤ºè¯
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(user_question, context)
            
            enhanced_prompt = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # 5. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
            llm_start = time.time()
            answer = self.llm.invoke(enhanced_prompt)
            llm_time = int((time.time() - llm_start) * 1000)
            
            if not answer or not answer.strip():
                return "âŒ LLMæœªèƒ½ç”Ÿæˆæœ‰æ•ˆç­”æ¡ˆï¼Œè¯·ç¨åé‡è¯•"
            
            # 6. æ„å»ºæœ€ç»ˆå›ç­”
            final_answer = self._format_final_answer(
                question=user_question,
                answer=answer.strip(),
                citations=citations if include_citations else None,
                search_time=search_time,
                llm_time=llm_time,
                avg_score=total_score / len(results) if results else 0
            )
            
            return final_answer
            
        except Exception as e:
            return f"âŒ æ™ºèƒ½é—®ç­”å¤±è´¥: {str(e)}\nğŸ’¡ è¯·æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€æˆ–ç¨åé‡è¯•"
    
    def _clean_content_for_context(self, content: str) -> str:
        """æ¸…ç†å†…å®¹ç”¨äºä¸Šä¸‹æ–‡"""
        # ç§»é™¤è¿‡å¤šçš„æ¢è¡Œå’Œç©ºæ ¼
        content = " ".join(content.split())
        # æˆªæ–­è¿‡é•¿å†…å®¹
        if len(content) > 300:
            content = content[:300] + "..."
        return content
    
    def _smart_truncate_context(self, context: str, max_chars: int) -> str:
        """æ™ºèƒ½æˆªæ–­ä¸Šä¸‹æ–‡ï¼Œä¿æŒæ®µè½å®Œæ•´æ€§"""
        if len(context) <= max_chars:
            return context
        
        # å¯»æ‰¾æœ€è¿‘çš„æ®µè½åˆ†éš”ç¬¦
        truncated = context[:max_chars]
        last_break = truncated.rfind("\n\n")
        
        if last_break > max_chars * 0.7:  # å¦‚æœæ–­ç‚¹ä½ç½®åˆç†
            return truncated[:last_break] + "\n\n[...æ›´å¤šå†…å®¹è¢«æˆªæ–­]"
        else:
            return truncated[:max_chars-20] + "...[å†…å®¹è¢«æˆªæ–­]"
    
    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        return (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š\n"
            "1. ğŸ“– ç²¾å‡†ç†è§£ï¼šä»”ç»†ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾\n"
            "2. ğŸ¯ å¯ä¿¡å›ç­”ï¼šä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œä¸ç¼–é€ å†…å®¹\n"
            "3. ğŸ” ä¿¡æ¯æ•´åˆï¼šä»å¤šä¸ªç‰‡æ®µä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå½¢æˆå®Œæ•´ç­”æ¡ˆ\n"
            "4. ğŸ’¡ æ¸…æ™°è¡¨è¾¾ï¼šç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”ï¼Œé€‚å½“ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼\n"
            "5. ğŸš« è¯šå®è¡¨è¾¾ï¼šå¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦è¯šè¯´æ˜\n\n"
            "å›ç­”æ ¼å¼è¦æ±‚ï¼š\n"
            "â€¢ ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜\n"
            "â€¢ å¿…è¦æ—¶ä½¿ç”¨è¦ç‚¹æˆ–æ­¥éª¤\n"
            "â€¢ å¼•ç”¨å…³é”®åŸæ–‡æ—¶ä½¿ç”¨å¼•å·\n"
            "â€¢ é¿å…é‡å¤å’Œå†—ä½™"
        )
    
    def _build_user_prompt(self, question: str, context: str) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        return (
            f"è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n\n"
            f"ã€é—®é¢˜ã€‘{question}\n\n"
            f"ã€ç›¸å…³ä¸Šä¸‹æ–‡ã€‘\n{context}\n\n"
            f"ã€è¦æ±‚ã€‘è¯·æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜éœ€è¦ä»€ä¹ˆé¢å¤–ä¿¡æ¯ã€‚"
        )
    
    def _format_final_answer(self, question: str, answer: str, citations: Optional[List[Dict]] = None, search_time: int = 0, llm_time: int = 0, avg_score: float = 0) -> str:
        """æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆ"""
        result = [f"ğŸ¤– **æ™ºèƒ½é—®ç­”ç»“æœ**\n"]
        result.append(answer)
        
        if citations:
            result.append("\n\nğŸ“š **å‚è€ƒæ¥æº**")
            for citation in citations:
                score_emoji = "ğŸŸ¢" if citation["score"] > 0.8 else "ğŸŸ¡" if citation["score"] > 0.6 else "ğŸ”µ"
                result.append(f"{score_emoji} [{citation['index']}] {citation['source']} (ç›¸ä¼¼åº¦: {citation['score']:.3f})")
        
        # æ·»åŠ æ€§èƒ½ä¿¡æ¯ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰
        result.append(f"\nâš¡ æ£€ç´¢: {search_time}ms | ç”Ÿæˆ: {llm_time}ms | å¹³å‡ç›¸ä¼¼åº¦: {avg_score:.3f}")
        
        return "\n".join(result)

    def _clear_knowledge_base(self, confirm: bool = False, namespace: Optional[str] = None, **kwargs) -> str:
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            if not confirm:
                return (
                    "âš ï¸ å±é™©æ“ä½œï¼šæ¸…ç©ºçŸ¥è¯†åº“å°†åˆ é™¤æ‰€æœ‰æ•°æ®ï¼\n"
                    "è¯·ä½¿ç”¨ confirm=true å‚æ•°ç¡®è®¤æ‰§è¡Œã€‚"
                )
            
            pipeline = self._get_pipeline(namespace)
            store = pipeline.get("store")
            namespace_id = pipeline.get("namespace", self.rag_namespace)
            success = store.clear_collection() if store else False
            
            if success:
                # é‡æ–°åˆå§‹åŒ–è¯¥å‘½åç©ºé—´
                self._pipelines[namespace_id] = create_rag_pipeline(
                    qdrant_url=self.qdrant_url,
                    qdrant_api_key=self.qdrant_api_key,
                    collection_name=self.collection_name,
                    rag_namespace=namespace_id
                )
                return f"âœ… çŸ¥è¯†åº“å·²æˆåŠŸæ¸…ç©ºï¼ˆå‘½åç©ºé—´ï¼š{namespace_id}ï¼‰"
            else:
                return "âŒ æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥"
            
        except Exception as e:
            return f"âŒ æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}"

    def _get_stats(self, namespace: Optional[str] = None) -> str:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡"""
        try:
            pipeline = self._get_pipeline(namespace)
            stats = pipeline["get_stats"]()
            
            stats_info = [
                "ğŸ“Š **RAG çŸ¥è¯†åº“ç»Ÿè®¡**",
                f"ğŸ“ å‘½åç©ºé—´: {pipeline.get('namespace', self.rag_namespace)}",
                f"ğŸ“‹ é›†åˆåç§°: {self.collection_name}",
                f"ğŸ“‚ å­˜å‚¨æ ¹è·¯å¾„: {self.knowledge_base_path}"
            ]
            
            # æ·»åŠ å­˜å‚¨ç»Ÿè®¡
            if stats:
                store_type = stats.get("store_type", "unknown")
                total_vectors = (
                    stats.get("points_count") or 
                    stats.get("vectors_count") or 
                    stats.get("count") or 0
                )
                
                stats_info.extend([
                    f"ğŸ“¦ å­˜å‚¨ç±»å‹: {store_type}",
                    f"ğŸ“Š æ–‡æ¡£åˆ†å—æ•°: {int(total_vectors)}",
                ])
                
                if "config" in stats:
                    config = stats["config"]
                    if isinstance(config, dict):
                        vector_size = config.get("vector_size", "unknown")
                        distance = config.get("distance", "unknown")
                        stats_info.extend([
                            f"ğŸ”¢ å‘é‡ç»´åº¦: {vector_size}",
                            f"ğŸ“ è·ç¦»åº¦é‡: {distance}"
                        ])
            
            # æ·»åŠ ç³»ç»ŸçŠ¶æ€
            stats_info.extend([
                "",
                "ğŸŸ¢ **ç³»ç»ŸçŠ¶æ€**",
                f"âœ… RAG ç®¡é“: {'æ­£å¸¸' if self.initialized else 'å¼‚å¸¸'}",
                f"âœ… LLM è¿æ¥: {'æ­£å¸¸' if hasattr(self, 'llm') else 'å¼‚å¸¸'}"
            ])
            
            return "\n".join(stats_info)
            
        except Exception as e:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"

    def get_relevant_context(self, query: str, limit: int = 3, max_chars: int = 1200, namespace: Optional[str] = None) -> str:
        """ä¸ºæŸ¥è¯¢è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        
        è¿™ä¸ªæ–¹æ³•å¯ä»¥è¢«Agentè°ƒç”¨æ¥è·å–ç›¸å…³çš„çŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        """
        try:
            if not query:
                return ""
            
            # ä½¿ç”¨ç»Ÿä¸€ RAG ç®¡é“æœç´¢
            pipeline = self._get_pipeline(namespace)
            results = pipeline["search"](
                query=query,
                top_k=limit
            )
            
            if not results:
                return ""
            
            # åˆå¹¶ä¸Šä¸‹æ–‡
            context_parts = []
            for result in results:
                content = result.get("metadata", {}).get("content", "")
                if content:
                    context_parts.append(content)
            
            merged_context = "\n\n".join(context_parts)
            
            # é™åˆ¶é•¿åº¦
            if len(merged_context) > max_chars:
                merged_context = merged_context[:max_chars] + "..."
            
            return merged_context
            
        except Exception as e:
            return f"è·å–ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"
    
    def batch_add_texts(self, texts: List[str], document_ids: Optional[List[str]] = None, chunk_size: int = 800, chunk_overlap: int = 100, namespace: Optional[str] = None) -> str:
        """æ‰¹é‡æ·»åŠ æ–‡æœ¬"""
        try:
            if not texts:
                return "âŒ æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º"
            
            if document_ids and len(document_ids) != len(texts):
                return "âŒ æ–‡æœ¬æ•°é‡å’Œæ–‡æ¡£IDæ•°é‡ä¸åŒ¹é…"
            
            pipeline = self._get_pipeline(namespace)
            t0 = time.time()
            
            total_chunks = 0
            successful_files = []
            
            for i, text in enumerate(texts):
                if not text or not text.strip():
                    continue
                    
                doc_id = document_ids[i] if document_ids else f"batch_text_{i}"
                tmp_path = os.path.join(self.knowledge_base_path, f"{doc_id}.md")
                
                try:
                    with open(tmp_path, 'w', encoding='utf-8') as f:
                        f.write(text)
                    
                    chunks_added = pipeline["add_documents"](
                        file_paths=[tmp_path],
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap
                    )
                    
                    total_chunks += chunks_added
                    successful_files.append(doc_id)
                    
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        if os.path.exists(tmp_path):
                            os.remove(tmp_path)
                    except Exception:
                        pass
            
            t1 = time.time()
            process_ms = int((t1 - t0) * 1000)
            
            return (
                f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆ\n"
                f"ğŸ“Š æˆåŠŸæ–‡ä»¶: {len(successful_files)}/{len(texts)}\n"
                f"ğŸ“Š æ€»åˆ†å—æ•°: {total_chunks}\n"
                f"â±ï¸ å¤„ç†æ—¶é—´: {process_ms}ms"
            )
            
        except Exception as e:
            return f"âŒ æ‰¹é‡æ·»åŠ å¤±è´¥: {str(e)}"
    
    def clear_all_namespaces(self) -> str:
        """æ¸…ç©ºå½“å‰å·¥å…·ç®¡ç†çš„æ‰€æœ‰å‘½åç©ºé—´æ•°æ®"""
        try:
            for ns, pipeline in self._pipelines.items():
                store = pipeline.get("store")
                if store:
                    store.clear_collection()
            self._pipelines.clear()
            # é‡æ–°åˆå§‹åŒ–é»˜è®¤å‘½åç©ºé—´
            self._init_components()
            return "âœ… æ‰€æœ‰å‘½åç©ºé—´æ•°æ®å·²æ¸…ç©ºå¹¶é‡æ–°åˆå§‹åŒ–"
        except Exception as e:
            return f"âŒ æ¸…ç©ºæ‰€æœ‰å‘½åç©ºé—´å¤±è´¥: {str(e)}"
    
    # ========================================
    # ä¾¿æ·æ¥å£æ–¹æ³•ï¼ˆç®€åŒ–ç”¨æˆ·è°ƒç”¨ï¼‰
    # ========================================
    
    def add_document(self, file_path: str, namespace: str = "default") -> str:
        """ä¾¿æ·æ–¹æ³•ï¼šæ·»åŠ å•ä¸ªæ–‡æ¡£"""
        return self.run({
            "action": "add_document",
            "file_path": file_path,
            "namespace": namespace
        })
    
    def add_text(self, text: str, namespace: str = "default", document_id: str = None) -> str:
        """ä¾¿æ·æ–¹æ³•ï¼šæ·»åŠ æ–‡æœ¬å†…å®¹"""
        return self.run({
            "action": "add_text",
            "text": text,
            "namespace": namespace,
            "document_id": document_id
        })
    
    def ask(self, question: str, namespace: str = "default", **kwargs) -> str:
        """ä¾¿æ·æ–¹æ³•ï¼šæ™ºèƒ½é—®ç­”"""
        params = {
            "action": "ask",
            "question": question,
            "namespace": namespace
        }
        params.update(kwargs)
        return self.run(params)
    
    def search(self, query: str, namespace: str = "default", **kwargs) -> str:
        """ä¾¿æ·æ–¹æ³•ï¼šæœç´¢çŸ¥è¯†åº“"""
        params = {
            "action": "search",
            "query": query,
            "namespace": namespace
        }
        params.update(kwargs)
        return self.run(params)
    
    def add_documents_batch(self, file_paths: List[str], namespace: str = "default") -> str:
        """æ‰¹é‡æ·»åŠ å¤šä¸ªæ–‡æ¡£"""
        if not file_paths:
            return "âŒ æ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º"
        
        results = []
        successful = 0
        failed = 0
        total_chunks = 0
        start_time = time.time()
        
        for i, file_path in enumerate(file_paths, 1):
            print(f"ğŸ“„ å¤„ç†æ–‡æ¡£ {i}/{len(file_paths)}: {os.path.basename(file_path)}")
            
            try:
                result = self.add_document(file_path, namespace)
                if "âœ…" in result:
                    successful += 1
                    # æå–åˆ†å—æ•°é‡
                    if "åˆ†å—æ•°é‡:" in result:
                        chunks = int(result.split("åˆ†å—æ•°é‡: ")[1].split("\n")[0])
                        total_chunks += chunks
                else:
                    failed += 1
                    results.append(f"âŒ {os.path.basename(file_path)}: å¤„ç†å¤±è´¥")
            except Exception as e:
                failed += 1
                results.append(f"âŒ {os.path.basename(file_path)}: {str(e)}")
        
        process_time = int((time.time() - start_time) * 1000)
        
        summary = [
            "ğŸ“Š **æ‰¹é‡å¤„ç†å®Œæˆ**",
            f"âœ… æˆåŠŸ: {successful}/{len(file_paths)} ä¸ªæ–‡æ¡£",
            f"ğŸ“Š æ€»åˆ†å—æ•°: {total_chunks}",
            f"â±ï¸ æ€»è€—æ—¶: {process_time}ms",
            f"ğŸ“ å‘½åç©ºé—´: {namespace}"
        ]
        
        if failed > 0:
            summary.append(f"âŒ å¤±è´¥: {failed} ä¸ªæ–‡æ¡£")
            summary.append("\n**å¤±è´¥è¯¦æƒ…:**")
            summary.extend(results)
        
        return "\n".join(summary)
    
    def add_texts_batch(self, texts: List[str], namespace: str = "default", document_ids: Optional[List[str]] = None) -> str:
        """æ‰¹é‡æ·»åŠ å¤šä¸ªæ–‡æœ¬"""
        if not texts:
            return "âŒ æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º"
        
        if document_ids and len(document_ids) != len(texts):
            return "âŒ æ–‡æœ¬æ•°é‡å’Œæ–‡æ¡£IDæ•°é‡ä¸åŒ¹é…"
        
        results = []
        successful = 0
        failed = 0
        total_chunks = 0
        start_time = time.time()
        
        for i, text in enumerate(texts):
            doc_id = document_ids[i] if document_ids else f"batch_text_{i+1}"
            print(f"ğŸ“ å¤„ç†æ–‡æœ¬ {i+1}/{len(texts)}: {doc_id}")
            
            try:
                result = self.add_text(text, namespace, doc_id)
                if "âœ…" in result:
                    successful += 1
                    # æå–åˆ†å—æ•°é‡
                    if "åˆ†å—æ•°é‡:" in result:
                        chunks = int(result.split("åˆ†å—æ•°é‡: ")[1].split("\n")[0])
                        total_chunks += chunks
                else:
                    failed += 1
                    results.append(f"âŒ {doc_id}: å¤„ç†å¤±è´¥")
            except Exception as e:
                failed += 1
                results.append(f"âŒ {doc_id}: {str(e)}")
        
        process_time = int((time.time() - start_time) * 1000)
        
        summary = [
            "ğŸ“Š **æ‰¹é‡æ–‡æœ¬å¤„ç†å®Œæˆ**",
            f"âœ… æˆåŠŸ: {successful}/{len(texts)} ä¸ªæ–‡æœ¬",
            f"ğŸ“Š æ€»åˆ†å—æ•°: {total_chunks}",
            f"â±ï¸ æ€»è€—æ—¶: {process_time}ms",
            f"ğŸ“ å‘½åç©ºé—´: {namespace}"
        ]
        
        if failed > 0:
            summary.append(f"âŒ å¤±è´¥: {failed} ä¸ªæ–‡æœ¬")
            summary.append("\n**å¤±è´¥è¯¦æƒ…:**")
            summary.extend(results)
        
        return "\n".join(summary)
